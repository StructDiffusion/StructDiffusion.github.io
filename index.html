<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="StructDiffusion: Object-Centric Diffusion for Semantic Rearrangement of Novel Objects">
  <meta name="keywords" content="Diffusion, Semantic Rearrangement, Transformer, Manipulation, Language Grounding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>StructDiffusion: Object-Centric Diffusion for Semantic Rearrangement of Novel Objects</title>

  <!--&lt;!&ndash; Global site tag (gtag.js) - Google Analytics &ndash;&gt;-->
  <!--<script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>-->
  <!--<script>-->
    <!--window.dataLayer = window.dataLayer || [];-->

    <!--function gtag() {-->
      <!--dataLayer.push(arguments);-->
    <!--}-->

    <!--gtag('js', new Date());-->

    <!--gtag('config', 'G-PYVRSFMDRL');-->
  <!--</script>-->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!--<link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!--<nav class="navbar" role="navigation" aria-label="main navigation">-->
  <!--<div class="navbar-brand">-->
    <!--<a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">-->
      <!--<span aria-hidden="true"></span>-->
      <!--<span aria-hidden="true"></span>-->
      <!--<span aria-hidden="true"></span>-->
    <!--</a>-->
  <!--</div>-->
  <!--<div class="navbar-menu">-->
    <!--<div class="navbar-start" style="flex-grow: 1; justify-content: center;">-->
      <!--<a class="navbar-item" href="https://keunhong.com">-->
      <!--<span class="icon">-->
          <!--<i class="fas fa-home"></i>-->
      <!--</span>-->
      <!--</a>-->

      <!--<div class="navbar-item has-dropdown is-hoverable">-->
        <!--<a class="navbar-link">-->
          <!--More Research-->
        <!--</a>-->
        <!--<div class="navbar-dropdown">-->
          <!--<a class="navbar-item" href="https://hypernerf.github.io">-->
            <!--HyperNeRF-->
          <!--</a>-->
          <!--<a class="navbar-item" href="https://nerfies.github.io">-->
            <!--Nerfies-->
          <!--</a>-->
          <!--<a class="navbar-item" href="https://latentfusion.github.io">-->
            <!--LatentFusion-->
          <!--</a>-->
          <!--<a class="navbar-item" href="https://photoshape.github.io">-->
            <!--PhotoShape-->
          <!--</a>-->
        <!--</div>-->
      <!--</div>-->
    <!--</div>-->

  <!--</div>-->
<!--</nav>-->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">StructDiffusion: Language-Guided <br> Creation of Physically-Valid Structures <br> using Unseen Objects</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="http://weiyuliu.com/">Weiyu Liu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yilundu.github.io/">Yilun Du</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a><sup>3,</sup><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://faculty.cc.gatech.edu/~chernova/">Sonia Chernova</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://cpaxton.github.io/about/">Chris Paxton</a><sup>5</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Georgia Tech,</span>
            <span class="author-block"><sup>2</sup>MIT,</span>
            <span class="author-block"><sup>3</sup>University of Utah,</span>
            <span class="author-block"><sup>4</sup>NVIDIA,</span>
            <span class="author-block"><sup>5</sup>Meta AI</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2211.04604.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2211.04604"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!--&lt;!&ndash; Video Link. &ndash;&gt;-->
              <!--<span class="link-block">-->
                <!--<a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
                   <!--class="external-link button is-normal is-rounded is-dark">-->
                  <!--<span class="icon">-->
                      <!--<i class="fab fa-youtube"></i>-->
                  <!--</span>-->
                  <!--<span>Video</span>-->
                <!--</a>-->
              <!--</span>-->
              <!--Code Link.-->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (stay tuned!)</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="supplementary/index.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <!--<span class="icon">-->
                      <!--<i class="fas fa-file-pdf"></i>-->
                  <!--</span>-->
                  <span>Supplementary</span>
                  </a>
              </span>
               <!--Dataset Link. -->
              <!--<span class="link-block">-->
                <!--<a href="https://github.com/google/nerfies/releases/tag/0.1"-->
                   <!--class="external-link button is-normal is-rounded is-dark">-->
                  <!--<span class="icon">-->
                      <!--<i class="far fa-images"></i>-->
                  <!--</span>-->
                  <!--<span>Data</span>-->
                  <!--</a>-->
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robots operating in human environments must be able to rearrange objects into semantically-meaningful
            configurations, even if these objects are previously unseen.
            We focus on the problem of building physically-valid structures without step-by-step instructions.
          </p>
          <p>
            We propose <span class="dnerf">StructDiffusion</span>, which combines a diffusion model and an object-centric transformer to construct
            structures given partial-view point clouds and high-level language goals, such as <i>"set the table"</i> and <i>"make a line"</i>.
          </p>
          <p>
            <span class="dnerf">StructDiffusion</span> improves success rate on assembling physically-valid structures out of unseen objects
            by on average 16% over an existing multi-modal transformer model, while allowing us to use one
            multi-task model to produce a wider range of different structures. We show experiments on held-out objects
            in both simulation and on real-world rearrangement tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content">
          <video id="teaser" controls playsinline height="100%">
            <source src="media/overview.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">
    <div class="rows">
    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="dnerf">StructDiffusion</span></h2>

        <!-- Interpolating. -->
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        <p>
          Building structures requires models that can reason about different constraints over where
          objects should be at once (e.g., object geometry, language-driven task semantics, physics)
          and generate solutions that respect all these constraints.
        </p>
        <br>
        <p>
          <span class="dnerf">StructDiffusion</span> consists of:
          (1) an <i>object-centric, language-conditioned diffusion model</i>, which learns how to construct different types of multi-object structures from observations of novel objects and language instructions,
          and (2) a <i>learned discriminator</i>, which drastically improves performance by rejecting samples violating physical constraints.
        </p>
        <br>
        <!--<p>-->
          <!--We handle these constraints in two ways: first, we train a <i>language-conditioned object-centric diffusion model</i> from which we can simultaneously sample goal poses for multiple objects;-->
          <!--and second, we train a <i>discriminator</i> model that looks at the imagined scenes to reject unrealistic samples.-->
        <!--</p>-->
        <br>

        <h3 class="title is-4">Generating Diverse Goals with Object-Centric Diffusion</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        <p>We use unknown object instance segmentation to break our scene up into objects, as per prior work (e.g., <a href="https://arxiv.org/abs/2108.12062">[1]</a>, <a href="https://arxiv.org/abs/2106.01352">[2]</a>, <a href="https://arxiv.org/abs/2108.12062">[3]</a>).
        Then, we use a multi-modal transformer to combine both word tokens and object encodings from <a href="https://arxiv.org/abs/2012.09688">Point Cloud Transformer</a> in order to make 6-DoF goal pose predictions.</p>
        <br>
        <img src="media/architecture.png" class="interpolation-image"
         alt="Architecture." />
        <br>
        <p>Our diffusion model is integrated with a transformer model that maintains an individual attention stream for each object. This object-centric approach allows us to focus on learning the interactions between objects based on their geometric features as well as the grounding of abstract concepts on spatio-semantic relations between objects (e.g., <i>large</i>, <i>circle</i>, <i>top</i>).</p>
        <br>
        <img src="media/diffusion_example.png" class="interpolation-image"
         alt="Architecture." />
        <br>
        <p>We start from the last step of the reverse diffusion process and jointly predict goal poses for all objects in the scene. This allows our model to reason about object-object interactions in a generalizable way, which outperforms simply predicting goal poses from multi-modal inputs.</p>
      </div>
    </div>
  </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">

    <h2 class="title is-3">Real-World Rearrangements</h2>

    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <video id="dollyzoom" autoplay controls muted playsinline height="100%">
            <source src="media/table_setting_1.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <video id="dollyzoom" autoplay controls muted playsinline height="100%">
            <source src="media/table_setting_2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

    </div>


    <div class="columns is-centered has-text-centered">
        <div class="content">
          <video id="teaser" autoplay controls muted playsinline height="100%">
            <source src="media/initial_position.mp4"
                    type="video/mp4">
          </video>
        </div>
    </div>

        <div class="columns is-centered has-text-centered">
        <div class="content">
          <video id="teaser" autoplay controls muted playsinline height="100%">
            <source src="media/different_structure.mp4"
                    type="video/mp4">
          </video>
        </div>
    </div>

        <div class="columns is-centered has-text-centered">
        <div class="content">
          <video id="teaser" autoplay controls muted playsinline height="100%">
            <source src="media/same_structure.mp4"
                    type="video/mp4">
          </video>
        </div>
    </div>

  </div>
</section>



<!--<section class="hero is-light is-small">-->
  <!--<div class="hero-body">-->
    <!--<div class="container">-->
      <!--<div id="results-carousel" class="carousel results-carousel">-->
        <!--<div class="item item-steve">-->
          <!--<video poster="" id="steve" autoplay controls muted loop playsinline height="100%">-->
            <!--<source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/steve.mp4"-->
                    <!--type="video/mp4">-->
          <!--</video>-->
        <!--</div>-->
        <!--<div class="item item-chair-tp">-->
          <!--<video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">-->
            <!--<source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/chair-tp.mp4"-->
                    <!--type="video/mp4">-->
          <!--</video>-->
        <!--</div>-->
        <!--<div class="item item-shiba">-->
          <!--<video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">-->
            <!--<source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/shiba.mp4"-->
                    <!--type="video/mp4">-->
          <!--</video>-->
        <!--</div>-->
        <!--<div class="item item-fullbody">-->
          <!--<video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">-->
            <!--<source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/fullbody.mp4"-->
                    <!--type="video/mp4">-->
          <!--</video>-->
        <!--</div>-->
        <!--<div class="item item-blueshirt">-->
          <!--<video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">-->
            <!--<source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/blueshirt.mp4"-->
                    <!--type="video/mp4">-->
          <!--</video>-->
        <!--</div>-->
        <!--<div class="item item-mask">-->
          <!--<video poster="" id="mask" autoplay controls muted loop playsinline height="100%">-->
            <!--<source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/mask.mp4"-->
                    <!--type="video/mp4">-->
          <!--</video>-->
        <!--</div>-->
        <!--<div class="item item-coffee">-->
          <!--<video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">-->
            <!--<source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/coffee.mp4"-->
                    <!--type="video/mp4">-->
          <!--</video>-->
        <!--</div>-->
        <!--<div class="item item-toby">-->
          <!--<video poster="" id="toby" autoplay controls muted loop playsinline height="100%">-->
            <!--<source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/toby2.mp4"-->
                    <!--type="video/mp4">-->
          <!--</video>-->
        <!--</div>-->
      <!--</div>-->
    <!--</div>-->
  <!--</div>-->
<!--</section>-->


<!--<section class="section">-->
  <!--<div class="container is-max-desktop">-->
    <!--&lt;!&ndash; Abstract. &ndash;&gt;-->
    <!--<div class="columns is-centered has-text-centered">-->
      <!--<div class="column is-four-fifths">-->
        <!--<h2 class="title is-3">Abstract</h2>-->
        <!--<div class="content has-text-justified">-->
          <!--<p>-->
            <!--We present the first method capable of photorealistically reconstructing a non-rigidly-->
            <!--deforming scene using photos/videos captured casually from mobile phones.-->
          <!--</p>-->
          <!--<p>-->
            <!--Our approach augments neural radiance fields-->
            <!--(NeRF) by optimizing an-->
            <!--additional continuous volumetric deformation field that warps each observed point into a-->
            <!--canonical 5D NeRF.-->
            <!--We observe that these NeRF-like deformation fields are prone to local minima, and-->
            <!--propose a coarse-to-fine optimization method for coordinate-based models that allows for-->
            <!--more robust optimization.-->
            <!--By adapting principles from geometry processing and physical simulation to NeRF-like-->
            <!--models, we propose an elastic regularization of the deformation field that further-->
            <!--improves robustness.-->
          <!--</p>-->
          <!--<p>-->
            <!--We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie-->
            <!--photos/videos into deformable NeRF-->
            <!--models that allow for photorealistic renderings of the subject from arbitrary-->
            <!--viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data-->
            <!--using a-->
            <!--rig with two mobile phones that take time-synchronized photos, yielding train/validation-->
            <!--images of the same pose at different viewpoints. We show that our method faithfully-->
            <!--reconstructs non-rigidly deforming scenes and reproduces unseen views with high-->
            <!--fidelity.-->
          <!--</p>-->
        <!--</div>-->
      <!--</div>-->
    <!--</div>-->
    <!--&lt;!&ndash;/ Abstract. &ndash;&gt;-->

    <!--&lt;!&ndash; Paper video. &ndash;&gt;-->
    <!--<div class="columns is-centered has-text-centered">-->
      <!--<div class="column is-four-fifths">-->
        <!--<h2 class="title is-3">Video</h2>-->
        <!--<div class="publication-video">-->
          <!--<iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"-->
                  <!--frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
        <!--</div>-->
      <!--</div>-->
    <!--</div>-->
    <!--&lt;!&ndash;/ Paper video. &ndash;&gt;-->
  <!--</div>-->
<!--</section>-->


<!--<section class="section">-->
  <!--<div class="container is-max-desktop">-->

    <!--<div class="columns is-centered">-->

      <!--&lt;!&ndash; Visual Effects. &ndash;&gt;-->
      <!--<div class="column">-->
        <!--<div class="content">-->
          <!--<h2 class="title is-3">Visual Effects</h2>-->
          <!--<p>-->
            <!--Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect-->
            <!--would be impossible without nerfies since it would require going through a wall.-->
          <!--</p>-->
          <!--<video id="dollyzoom" autoplay controls muted loop playsinline height="100%">-->
            <!--<source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/dollyzoom-stacked.mp4"-->
                    <!--type="video/mp4">-->
          <!--</video>-->
        <!--</div>-->
      <!--</div>-->
      <!--&lt;!&ndash;/ Visual Effects. &ndash;&gt;-->

      <!--&lt;!&ndash; Matting. &ndash;&gt;-->
      <!--<div class="column">-->
        <!--<h2 class="title is-3">Matting</h2>-->
        <!--<div class="columns is-centered">-->
          <!--<div class="column content">-->
            <!--<p>-->
              <!--As a byproduct of our method, we can also solve the matting problem by ignoring-->
              <!--samples that fall outside of a bounding box during rendering.-->
            <!--</p>-->
            <!--<video id="matting-video" controls playsinline height="100%">-->
              <!--<source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/matting.mp4"-->
                      <!--type="video/mp4">-->
            <!--</video>-->
          <!--</div>-->

        <!--</div>-->
      <!--</div>-->
    <!--</div>-->
    <!--&lt;!&ndash;/ Matting. &ndash;&gt;-->

    <!--&lt;!&ndash; Animation. &ndash;&gt;-->
    <!--<div class="columns is-centered">-->
      <!--<div class="column is-full-width">-->
        <!--<h2 class="title is-3">Animation</h2>-->

        <!--&lt;!&ndash; Interpolating. &ndash;&gt;-->
        <!--<h3 class="title is-4">Interpolating states</h3>-->
        <!--<div class="content has-text-justified">-->
          <!--<p>-->
            <!--We can also animate the scene by interpolating the deformation latent codes of two input-->
            <!--frames. Use the slider here to linearly interpolate between the left frame and the right-->
            <!--frame.-->
          <!--</p>-->
        <!--</div>-->
        <!--<div class="columns is-vcentered interpolation-panel">-->
          <!--<div class="column is-3 has-text-centered">-->
            <!--<img src="https://homes.cs.washington.edu/~kpar/nerfies/images/interpolate_start.jpg"-->
                 <!--class="interpolation-image"-->
                 <!--alt="Interpolate start reference image."/>-->
            <!--<p>Start Frame</p>-->
          <!--</div>-->
          <!--<div class="column interpolation-video-column">-->
            <!--<div id="interpolation-image-wrapper">-->
              <!--Loading...-->
            <!--</div>-->
            <!--<input class="slider is-fullwidth is-large is-info"-->
                   <!--id="interpolation-slider"-->
                   <!--step="1" min="0" max="100" value="0" type="range">-->
          <!--</div>-->
          <!--<div class="column is-3 has-text-centered">-->
            <!--<img src="https://homes.cs.washington.edu/~kpar/nerfies/images/interpolate_end.jpg"-->
                 <!--class="interpolation-image"-->
                 <!--alt="Interpolation end reference image."/>-->
            <!--<p class="is-bold">End Frame</p>-->
          <!--</div>-->
        <!--</div>-->
        <!--<br/>-->
        <!--&lt;!&ndash;/ Interpolating. &ndash;&gt;-->

        <!--&lt;!&ndash; Re-rendering. &ndash;&gt;-->
        <!--<h3 class="title is-4">Re-rendering the input video</h3>-->
        <!--<div class="content has-text-justified">-->
          <!--<p>-->
            <!--Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel-->
            <!--viewpoint such as a stabilized camera by playing back the training deformations.-->
          <!--</p>-->
        <!--</div>-->
        <!--<div class="content has-text-centered">-->
          <!--<video id="replay-video"-->
                 <!--controls-->
                 <!--muted-->
                 <!--preload-->
                 <!--playsinline-->
                 <!--width="75%">-->
            <!--<source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/replay.mp4"-->
                    <!--type="video/mp4">-->
          <!--</video>-->
        <!--</div>-->
        <!--&lt;!&ndash;/ Re-rendering. &ndash;&gt;-->

      <!--</div>-->
    <!--</div>-->
    <!--&lt;!&ndash;/ Animation. &ndash;&gt;-->


    <!--&lt;!&ndash; Concurrent Work. &ndash;&gt;-->
    <!--<div class="columns is-centered">-->
      <!--<div class="column is-full-width">-->
        <!--<h2 class="title is-3">Related Links</h2>-->

        <!--<div class="content has-text-justified">-->
          <!--<p>-->
            <!--There's a lot of excellent work that was introduced around the same time as ours.-->
          <!--</p>-->
          <!--<p>-->
            <!--<a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.-->
          <!--</p>-->
          <!--<p>-->
            <!--<a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>-->
            <!--both use deformation fields to model non-rigid scenes.-->
          <!--</p>-->
          <!--<p>-->
            <!--Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>-->
          <!--</p>-->
          <!--<p>-->
            <!--There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.-->
          <!--</p>-->
        <!--</div>-->
      <!--</div>-->
    <!--</div>-->
    <!--&lt;!&ndash;/ Concurrent Work. &ndash;&gt;-->

  <!--</div>-->
<!--</section>-->


<!--<section class="section" id="BibTeX">-->
  <!--<div class="container is-max-desktop content">-->
    <!--<h2 class="title">BibTeX</h2>-->
    <!--<pre><code>@article{park2021nerfies,-->
  <!--author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},-->
  <!--title     = {Nerfies: Deformable Neural Radiance Fields},-->
  <!--journal   = {ICCV},-->
  <!--year      = {2021},-->
<!--}</code></pre>-->
  <!--</div>-->
<!--</section>-->


<footer class="footer">
  <div class="container">
    <!--<div class="content has-text-centered">-->
      <!--<a class="icon-link"-->
         <!--href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">-->
        <!--<i class="fas fa-file-pdf"></i>-->
      <!--</a>-->
      <!--<a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>-->
        <!--<i class="fab fa-github"></i>-->
      <!--</a>-->
    <!--</div>-->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by <a href="https://keunhong.com/">Keunhong Park</a>.
          </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
